{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6459a6b8",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bb8d9a-b4ba-41ef-ae45-6a325715724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "# Dataset hyper parameters\n",
    "HP_BALANCED = hp.HParam('balanced', hp.Discrete([0, 1])) # 0 unbalanced, 1 balanced\n",
    "HP_NORM_IMAGES = hp.HParam('norm_images', hp.Discrete([0, 1]))\n",
    "\n",
    "# Network hyper parameters\n",
    "HP_NETWORKS = hp.HParam('network', hp.Discrete(['Xception',\n",
    "                                            'VGG16',\n",
    "                                            'VGG19',\n",
    "                                            'ResNet50',\n",
    "                                            'ResNet50V2',\n",
    "                                            'ResNet101',\n",
    "                                            'ResNet101V2',\n",
    "                                            'ResNet152',\n",
    "                                            'ResNet152V2',\n",
    "                                            'InceptionV3',\n",
    "                                            'InceptionResNetV2',\n",
    "                                            'MobileNet',\n",
    "                                            'MobileNetV2',\n",
    "                                            'DenseNet121',\n",
    "                                            'DenseNet169',\n",
    "                                            'DenseNet201',\n",
    "                                            'NASNetMobile',\n",
    "                                            'NASNetLarge',\n",
    "                                            'EfficientNetB0',\n",
    "                                            'EfficientNetB1',\n",
    "                                            'EfficientNetB2',\n",
    "                                            'EfficientNetB3',\n",
    "                                            'EfficientNetB4',\n",
    "                                            'EfficientNetB5',\n",
    "                                            'EfficientNetB6',\n",
    "                                            'EfficientNetB7',\n",
    "                                            'EfficientNetV2B0',\n",
    "                                            'EfficientNetV2B1',\n",
    "                                            'EfficientNetV2B2',\n",
    "                                            'EfficientNetV2B3',\n",
    "                                            'EfficientNetV2S',\n",
    "                                            'EfficientNetV2M',\n",
    "                                            'EfficientNetV2L',\n",
    "                                            'ConvNeXtTiny',\n",
    "                                            'ConvNeXtSmall',\n",
    "                                            'ConvNeXtBase',\n",
    "                                            'ConvNeXtLarge',\n",
    "                                            'ConvNeXtXLarge' ])) \n",
    "\n",
    "\n",
    "HP_ACTIVATION_FUNCTIONS = hp.HParam('act_functions', hp.Discrete(['relu', 'selu', 'tanh']))\n",
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([16, 32, 64, 128]))\n",
    "HP_EARLY_STOP = hp.HParam('early_stop', hp.Discrete([3, 5, 7, 10, 15]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.0, 0.5))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbb98f8-3ab5-4e80-8249-a85f106b64df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)\n",
    "        result_metrics = train_test_model(hparams)\n",
    "        tf.summary.scalar(METRIC_NAMES, result_metrics, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a867ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "# Function for normalizing images\n",
    "def normImages(X):\n",
    "    for i, image in enumerate(X):\n",
    "        max_n = np.max(image)\n",
    "        image /= max_n\n",
    "        X[i] = np.abs(image)\n",
    "    return X\n",
    "\n",
    "# Obtaining training, validation and test data\n",
    "def train_val_test_split(balanced, normalized):\n",
    "    spectrogram = ['spectrogram', 'mel', 'mtf']\n",
    "    \n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    X_val = []\n",
    "\n",
    "    y_flag = 0 # y's has not been obtained yet \n",
    "    # root_dir = \"/home/polivares/scratch/Datasets/PowerLineFaults/\"\n",
    "    root_dir = '/home/polivares/Dropbox/Work/PostDoc/PowerLineFaults/'\n",
    "    \n",
    "    for sp in spectrogram:\n",
    "        X_full = np.load(f\"{root_dir}dataset_{sp}/full/images_full.npy\")[:3000]\n",
    "        \n",
    "        \n",
    "        if not y_flag:\n",
    "            y_flag = 1\n",
    "            \n",
    "            y_full = np.load(f\"{root_dir}dataset_{sp}/full/labels_full.npy\").reshape(-1)[:3000]\n",
    "            if balanced: # getting balanced data from index\n",
    "                # Index 1, partial discharge\n",
    "                index_1 = np.where(y_full==1)[0]\n",
    "                len_index_1 = len(index_1)\n",
    "                index_train_1, index_val_1, index_test_1 = index_1[:len_index_1//3], index_1[len_index_1//3:2*len_index_1//3], index_1[2*len_index_1//3:]\n",
    "\n",
    "                # Index 0, non partial discharge\n",
    "                index_0 = np.where(y_full==0)[0]\n",
    "                index_train_0, index_val_0, index_test_0 = index_0[:len_index_1//3], index_0[len_index_1//3:2*len_index_1//3], index_0[2*len_index_1//3:]\n",
    "\n",
    "                # Obtaining index\n",
    "                index_train = np.concatenate([index_train_0, index_train_1])\n",
    "                np.random.shuffle(index_train)\n",
    "                index_val = np.concatenate([index_val_0, index_val_1])\n",
    "                np.random.shuffle(index_val)\n",
    "                index_test = np.concatenate([index_test_0, index_test_1])\n",
    "                np.random.shuffle(index_test)\n",
    "\n",
    "            else: # Unbalanced data, similar to the original from index\n",
    "                index_full = np.arange(len(y_full))\n",
    "                np.random.shuffle(index_full)\n",
    "                len_index = 1000\n",
    "                \n",
    "                # Obtaining index\n",
    "                index_train, index_val, index_test = index_full[:len_index], index_full[len_index:2*len_index], index_full[2*len_index:]\n",
    "            \n",
    "            y_train = y_full[index_train]\n",
    "            y_val = y_full[index_val]\n",
    "            y_test = y_full[index_test]\n",
    "            \n",
    "            del y_full\n",
    "        \n",
    "        if normalized:\n",
    "            X_full = normImages(X_full)\n",
    "        \n",
    "        X_train.append(X_full[index_train])\n",
    "        X_val.append(X_full[index_val])\n",
    "        X_test.append(X_full[index_test])\n",
    "        \n",
    "        del X_full\n",
    "        \n",
    "    X_train_c = np.concatenate(X_train, axis=3) \n",
    "    X_val_c = np.concatenate(X_val, axis=3)\n",
    "    X_test_c = np.concatenate(X_test, axis=3)\n",
    "    \n",
    "    return X_train_c, y_train, X_val_c, y_val, X_test_c, y_test\n",
    "\n",
    "\n",
    "# X_train_c, y_train, X_val_c, y_val, X_test_c, y_test = train_val_test_split(balanced=0,normalized=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b90298d-418b-4677-80db-3eadb79bff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def train_test_model(hparams, METRICS):\n",
    "    # Model creation\n",
    "    print(\"Model creation\")\n",
    "    base_model = getattr(tf.keras.applications, hparams[HP_NETWORKS])(include_top=False, weights='imagenet', input_shape=(256, 256, 3))\n",
    "    x = tf.keras.layers.Conv2D(64, (3,3), activation=hparams[HP_ACTIVATION_FUNCTIONS])(base_model.output)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(x)\n",
    "    x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation=hparams[HP_ACTIVATION_FUNCTIONS])(x)\n",
    "    x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)\n",
    "    # avg = tf.keras.layers.GlobalAveragePooling2D()()\n",
    "    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = tf.keras.Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False # Esto impide que las capas se re entrenen\n",
    "    \n",
    "    # Compile model\n",
    "    print(\"Model compile\")\n",
    "    early_stopping = EarlyStopping(patience = hparams[HP_EARLY_STOP])\n",
    "    model.compile(loss = 'bce', optimizer = hparams[HP_OPTIMIZER], metrics=METRICS)\n",
    "    \n",
    "    # Obtaining data \n",
    "    print(\"Obtaining data\")\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(hparams[HP_BALANCED], hparams[HP_NORM_IMAGES])\n",
    "    \n",
    "    # Fitting training\n",
    "    print(\"Fitting training\")\n",
    "    history_model = model.fit(X_train, y_train, epochs=1000, \n",
    "                            validation_data=(X_val, y_val),\n",
    "                            batch_size=10,\n",
    "                            callbacks=[early_stopping])\n",
    "    # Evaluation on test\n",
    "    print(\"Evaluation on test\")\n",
    "    results = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    # Returning metrics results\n",
    "    print(\"Returning metrics results\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb88267a-bc7e-44cc-8d93-9ebcf11acd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation with hparams\n",
    "def run(run_dir, hparams):\n",
    "    METRICS = [\n",
    "          tf.keras.metrics.TruePositives(name='tp'),\n",
    "          tf.keras.metrics.FalsePositives(name='fp'),\n",
    "          tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "          tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "          tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          tf.keras.metrics.Precision(name='precision'),\n",
    "          tf.keras.metrics.Recall(name='recall'),\n",
    "          tf.keras.metrics.AUC(name='auc'),\n",
    "          tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "    \n",
    "    METRICS_NAMES = [\n",
    "        'loss',\n",
    "        'tp',\n",
    "        'fp',\n",
    "        'tn',\n",
    "        'fn',\n",
    "        'accuracy',\n",
    "        'precision',\n",
    "        'recall',\n",
    "        'auc',\n",
    "        'prc'\n",
    "    ]\n",
    "\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        results = train_test_model(hparams, METRICS)\n",
    "    \n",
    "        for name, metric in zip(METRICS_NAMES, results):\n",
    "            print(f\"Summary: metric {name} value {metric}\")\n",
    "            tf.summary.scalar(name, metric, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e423fb-c5e1-4cf3-bd5a-8ea6c81e7139",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_num = 0\n",
    "\n",
    "for balanced in HP_BALANCED.domain.values:\n",
    "    for norm_image in HP_NORM_IMAGES.domain.values:\n",
    "        for network in HP_NETWORKS.domain.values:\n",
    "            for act_func in HP_ACTIVATION_FUNCTIONS.domain.values:\n",
    "                for num_units in HP_NUM_UNITS.domain.values:\n",
    "                    for dropout in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "                        for early_stop in HP_EARLY_STOP.domain.values:\n",
    "                            for optimizer in HP_OPTIMIZER.domain.values:\n",
    "                                hparams = {\n",
    "                                     HP_BALANCED : balanced,\n",
    "                                     HP_NORM_IMAGES: norm_image,\n",
    "                                     HP_NETWORKS: network,\n",
    "                                     HP_ACTIVATION_FUNCTIONS: act_func,\n",
    "                                     HP_NUM_UNITS: num_units,\n",
    "                                     HP_DROPOUT: dropout,\n",
    "                                     HP_EARLY_STOP: early_stop,\n",
    "                                     HP_OPTIMIZER: optimizer\n",
    "                                }\n",
    "                                run_name = \"run-%d\" % session_num\n",
    "                                print('--- Starting trial: %s' % run_name)\n",
    "                                print({h.name: hparams[h] for h in hparams})\n",
    "                                run('logs/hparam_tuning/' + run_name, hparams)\n",
    "                                session_num += 1\n",
    "                                \n",
    "                            \n",
    "                        \n",
    "                    \n",
    "                \n",
    "            \n",
    "        \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
