{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6459a6b8",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39bb8d9a-b4ba-41ef-ae45-6a325715724b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-26 01:58:53.513598: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-26 01:58:53.677392: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-10-26 01:58:53.711609: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-26 01:58:54.346962: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:\n",
      "2022-10-26 01:58:54.347013: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:\n",
      "2022-10-26 01:58:54.347017: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "# Dataset hyper parameters\n",
    "HP_BALANCED = hp.HParam('balanced', hp.Discrete([0, 1])) # 0 unbalanced, 1 balanced\n",
    "HP_NORM_IMAGES = hp.HParam('norm_images', hp.Discrete([0, 1]))\n",
    "\n",
    "# Network hyper parameters\n",
    "HP_NETWORKS = hp.HParam('network', hp.Discrete(['Xception',\n",
    "                                            'VGG16',\n",
    "                                            'VGG19',\n",
    "                                            'ResNet50',\n",
    "                                            'ResNet50V2',\n",
    "                                            'ResNet101',\n",
    "                                            'ResNet101V2',\n",
    "                                            'ResNet152',\n",
    "                                            'ResNet152V2',\n",
    "                                            'InceptionV3',\n",
    "                                            'InceptionResNetV2',\n",
    "                                            'MobileNet',\n",
    "                                            'MobileNetV2',\n",
    "                                            'DenseNet121',\n",
    "                                            'DenseNet169',\n",
    "                                            'DenseNet201',\n",
    "                                            'NASNetMobile',\n",
    "                                            'NASNetLarge',\n",
    "                                            'EfficientNetB0',\n",
    "                                            'EfficientNetB1',\n",
    "                                            'EfficientNetB2',\n",
    "                                            'EfficientNetB3',\n",
    "                                            'EfficientNetB4',\n",
    "                                            'EfficientNetB5',\n",
    "                                            'EfficientNetB6',\n",
    "                                            'EfficientNetB7',\n",
    "                                            'EfficientNetV2B0',\n",
    "                                            'EfficientNetV2B1',\n",
    "                                            'EfficientNetV2B2',\n",
    "                                            'EfficientNetV2B3',\n",
    "                                            'EfficientNetV2S',\n",
    "                                            'EfficientNetV2M',\n",
    "                                            'EfficientNetV2L',\n",
    "                                            'ConvNeXtTiny',\n",
    "                                            'ConvNeXtSmall',\n",
    "                                            'ConvNeXtBase',\n",
    "                                            'ConvNeXtLarge',\n",
    "                                            'ConvNeXtXLarge' ])) \n",
    "\n",
    "\n",
    "HP_ACTIVATION_FUNCTIONS = hp.HParam('act_functions', hp.Discrete(['relu', 'selu', 'tanh']))\n",
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([16, 32, 64, 128]))\n",
    "HP_EARLY_STOP = hp.HParam('early_stop', hp.Discrete([3, 5, 7, 10, 15]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.0, 0.5))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "\n",
    "# METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "\n",
    "# hparams = [\n",
    "#      HP_BALANCED,\n",
    "#      HP_NORM_IMAGES,\n",
    "#      HP_NETWORKS,\n",
    "#      HP_ACTIVATION_FUNCTIONS,\n",
    "#      HP_NUM_UNITS,\n",
    "#      HP_DROPOUT,\n",
    "#      HP_EARLY_STOP,\n",
    "#      HP_OPTIMIZER\n",
    "# ]\n",
    "\n",
    "# metrics = [hp.Metric(x, group='train', display_name=f'{x} (train)') for x in METRICS_NAMES] + \\\n",
    "#           [hp.Metric(x, group='validation', display_name=f'{x} (validation)') for x in METRICS_NAMES]\n",
    "\n",
    "\n",
    "# with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "#     hp.hparams_config(\n",
    "#         hparams=hparams,\n",
    "#         # metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "#         metrics = metrics\n",
    "#       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bbb98f8-3ab5-4e80-8249-a85f106b64df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)\n",
    "        result_metrics = train_test_model(hparams)\n",
    "        tf.summary.scalar(METRIC_NAMES, result_metrics, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f305545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation \n",
    "from pyts.image import MarkovTransitionField\n",
    "import dask.dataframe as dd  \n",
    "import pandas as pd\n",
    "    \n",
    "\n",
    "from scipy import signal as sign\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a867ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "# Function for normalizing images\n",
    "def normImages(X):\n",
    "    for i, image in enumerate(X):\n",
    "        max_n = np.max(image)\n",
    "        image /= max_n\n",
    "        X[i] = np.abs(image)\n",
    "    return X\n",
    "\n",
    "# Obtaining training, validation and test data\n",
    "def train_val_test_split(balanced, normalized):\n",
    "    spectrogram = ['spectrogram', 'mel', 'mtf']\n",
    "    \n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    X_val = []\n",
    "\n",
    "    y_flag = 0 # y's has not been obtained yet \n",
    "    # root_dir = \"/home/polivares/scratch/Datasets/PowerLineFaults/\"\n",
    "    root_dir = '/home/polivares/Dropbox/Work/PostDoc/PowerLineFaults/'\n",
    "    \n",
    "    for sp in spectrogram:\n",
    "        X_full = np.load(f\"{root_dir}dataset_{sp}/full/images_full.npy\")[:3000]\n",
    "        \n",
    "        \n",
    "        if not y_flag:\n",
    "            y_flag = 1\n",
    "            \n",
    "            y_full = np.load(f\"{root_dir}dataset_{sp}/full/labels_full.npy\").reshape(-1)[:3000]\n",
    "            if balanced: # getting balanced data from index\n",
    "                # Index 1, partial discharge\n",
    "                index_1 = np.where(y_full==1)[0]\n",
    "                len_index_1 = len(index_1)\n",
    "                index_train_1, index_val_1, index_test_1 = index_1[:len_index_1//3], index_1[len_index_1//3:2*len_index_1//3], index_1[2*len_index_1//3:]\n",
    "\n",
    "                # Index 0, non partial discharge\n",
    "                index_0 = np.where(y_full==0)[0]\n",
    "                index_train_0, index_val_0, index_test_0 = index_0[:len_index_1//3], index_0[len_index_1//3:2*len_index_1//3], index_0[2*len_index_1//3:]\n",
    "\n",
    "                # Obtaining index\n",
    "                index_train = np.concatenate([index_train_0, index_train_1])\n",
    "                np.random.shuffle(index_train)\n",
    "                index_val = np.concatenate([index_val_0, index_val_1])\n",
    "                np.random.shuffle(index_val)\n",
    "                index_test = np.concatenate([index_test_0, index_test_1])\n",
    "                np.random.shuffle(index_test)\n",
    "\n",
    "            else: # Unbalanced data, similar to the original from index\n",
    "                index_full = np.arange(len(y_full))\n",
    "                np.random.shuffle(index_full)\n",
    "                len_index = 1000\n",
    "                \n",
    "                # Obtaining index\n",
    "                index_train, index_val, index_test = index_full[:len_index], index_full[len_index:2*len_index], index_full[2*len_index:]\n",
    "            \n",
    "            y_train = y_full[index_train]\n",
    "            y_val = y_full[index_val]\n",
    "            y_test = y_full[index_test]\n",
    "            \n",
    "            del y_full\n",
    "        \n",
    "        if normalized:\n",
    "            X_full = normImages(X_full)\n",
    "        \n",
    "        X_train.append(X_full[index_train])\n",
    "        X_val.append(X_full[index_val])\n",
    "        X_test.append(X_full[index_test])\n",
    "        \n",
    "        del X_full\n",
    "        \n",
    "    X_train_c = np.concatenate(X_train, axis=3) \n",
    "    X_val_c = np.concatenate(X_val, axis=3)\n",
    "    X_test_c = np.concatenate(X_test, axis=3)\n",
    "    \n",
    "    return X_train_c, y_train, X_val_c, y_val, X_test_c, y_test\n",
    "\n",
    "\n",
    "# X_train_c, y_train, X_val_c, y_val, X_test_c, y_test = train_val_test_split(balanced=0,normalized=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b90298d-418b-4677-80db-3eadb79bff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def train_test_model(hparams, METRICS):\n",
    "    # Model creation\n",
    "    print(\"Model creation\")\n",
    "    base_model = getattr(tf.keras.applications, hparams[HP_NETWORKS])(include_top=False, weights='imagenet', input_shape=(256, 256, 3))\n",
    "    x = tf.keras.layers.Conv2D(64, (3,3), activation=hparams[HP_ACTIVATION_FUNCTIONS])(base_model.output)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(x)\n",
    "    x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation=hparams[HP_ACTIVATION_FUNCTIONS])(x)\n",
    "    x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)\n",
    "    # avg = tf.keras.layers.GlobalAveragePooling2D()()\n",
    "    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = tf.keras.Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False # Esto impide que las capas se re entrenen\n",
    "    \n",
    "    # Compile model\n",
    "    print(\"Model compile\")\n",
    "    early_stopping = EarlyStopping(patience = hparams[HP_EARLY_STOP])\n",
    "    model.compile(loss = 'bce', optimizer = hparams[HP_OPTIMIZER], metrics=METRICS)\n",
    "    \n",
    "    # Obtaining data \n",
    "    print(\"Obtaining data\")\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(hparams[HP_BALANCED], hparams[HP_NORM_IMAGES])\n",
    "    \n",
    "    # Fitting training\n",
    "    print(\"Fitting training\")\n",
    "    history_model = model.fit(X_train, y_train, epochs=1000, \n",
    "                            validation_data=(X_val, y_val),\n",
    "                            batch_size=10,\n",
    "                            callbacks=[early_stopping])\n",
    "    # Evaluation on test\n",
    "    print(\"Evaluation on test\")\n",
    "    results = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    # Returning metrics results\n",
    "    print(\"Returning metrics results\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb88267a-bc7e-44cc-8d93-9ebcf11acd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation with hparams\n",
    "def run(run_dir, hparams):\n",
    "    METRICS = [\n",
    "          tf.keras.metrics.TruePositives(name='tp'),\n",
    "          tf.keras.metrics.FalsePositives(name='fp'),\n",
    "          tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "          tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "          tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          tf.keras.metrics.Precision(name='precision'),\n",
    "          tf.keras.metrics.Recall(name='recall'),\n",
    "          tf.keras.metrics.AUC(name='auc'),\n",
    "          tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "    \n",
    "    METRICS_NAMES = [\n",
    "        'loss',\n",
    "        'tp',\n",
    "        'fp',\n",
    "        'tn',\n",
    "        'fn',\n",
    "        'accuracy',\n",
    "        'precision',\n",
    "        'recall',\n",
    "        'auc',\n",
    "        'prc'\n",
    "    ]\n",
    "\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        results = train_test_model(hparams, METRICS)\n",
    "    \n",
    "        for name, metric in zip(METRICS_NAMES, results):\n",
    "            print(f\"Summary: metric {name} value {metric}\")\n",
    "            tf.summary.scalar(name, metric, step=1)\n",
    "    # return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21e423fb-c5e1-4cf3-bd5a-8ea6c81e7139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'balanced': 0, 'norm_images': 0, 'network': 'ConvNeXtBase', 'act_functions': 'relu', 'num_units': 16, 'dropout': 0.0, 'early_stop': 3, 'optimizer': 'adam'}\n",
      "Model creation\n",
      "Model compile\n",
      "Obtaining data\n",
      "Fitting training\n",
      "Epoch 1/1000\n",
      "100/100 [==============================] - 30s 247ms/step - loss: 0.4366 - tp: 10.0000 - fp: 33.0000 - tn: 897.0000 - fn: 60.0000 - accuracy: 0.9070 - precision: 0.2326 - recall: 0.1429 - auc: 0.6473 - prc: 0.1346 - val_loss: 0.2742 - val_tp: 18.0000 - val_fp: 98.0000 - val_tn: 843.0000 - val_fn: 41.0000 - val_accuracy: 0.8610 - val_precision: 0.1552 - val_recall: 0.3051 - val_auc: 0.7746 - val_prc: 0.1459\n",
      "Epoch 2/1000\n",
      "100/100 [==============================] - 23s 229ms/step - loss: 0.2161 - tp: 21.0000 - fp: 23.0000 - tn: 907.0000 - fn: 49.0000 - accuracy: 0.9280 - precision: 0.4773 - recall: 0.3000 - auc: 0.8207 - prc: 0.2938 - val_loss: 0.1940 - val_tp: 23.0000 - val_fp: 39.0000 - val_tn: 902.0000 - val_fn: 36.0000 - val_accuracy: 0.9250 - val_precision: 0.3710 - val_recall: 0.3898 - val_auc: 0.8359 - val_prc: 0.3010\n",
      "Epoch 3/1000\n",
      "100/100 [==============================] - 23s 233ms/step - loss: 0.1678 - tp: 29.0000 - fp: 24.0000 - tn: 906.0000 - fn: 41.0000 - accuracy: 0.9350 - precision: 0.5472 - recall: 0.4143 - auc: 0.8987 - prc: 0.4766 - val_loss: 0.1997 - val_tp: 21.0000 - val_fp: 32.0000 - val_tn: 909.0000 - val_fn: 38.0000 - val_accuracy: 0.9300 - val_precision: 0.3962 - val_recall: 0.3559 - val_auc: 0.8097 - val_prc: 0.2732\n",
      "Epoch 4/1000\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.1267 - tp: 33.0000 - fp: 14.0000 - tn: 916.0000 - fn: 37.0000 - accuracy: 0.9490 - precision: 0.7021 - recall: 0.4714 - auc: 0.9479 - prc: 0.6606 - val_loss: 0.2534 - val_tp: 10.0000 - val_fp: 18.0000 - val_tn: 923.0000 - val_fn: 49.0000 - val_accuracy: 0.9330 - val_precision: 0.3571 - val_recall: 0.1695 - val_auc: 0.7929 - val_prc: 0.2593\n",
      "Epoch 5/1000\n",
      "100/100 [==============================] - 24s 239ms/step - loss: 0.1153 - tp: 38.0000 - fp: 22.0000 - tn: 908.0000 - fn: 32.0000 - accuracy: 0.9460 - precision: 0.6333 - recall: 0.5429 - auc: 0.9628 - prc: 0.6699 - val_loss: 0.1882 - val_tp: 14.0000 - val_fp: 17.0000 - val_tn: 924.0000 - val_fn: 45.0000 - val_accuracy: 0.9380 - val_precision: 0.4516 - val_recall: 0.2373 - val_auc: 0.8431 - val_prc: 0.3466\n",
      "Epoch 6/1000\n",
      "100/100 [==============================] - 24s 239ms/step - loss: 0.0818 - tp: 48.0000 - fp: 9.0000 - tn: 921.0000 - fn: 22.0000 - accuracy: 0.9690 - precision: 0.8421 - recall: 0.6857 - auc: 0.9829 - prc: 0.8225 - val_loss: 0.2382 - val_tp: 4.0000 - val_fp: 7.0000 - val_tn: 934.0000 - val_fn: 55.0000 - val_accuracy: 0.9380 - val_precision: 0.3636 - val_recall: 0.0678 - val_auc: 0.8279 - val_prc: 0.3048\n",
      "Epoch 7/1000\n",
      "100/100 [==============================] - 24s 240ms/step - loss: 0.0657 - tp: 54.0000 - fp: 7.0000 - tn: 923.0000 - fn: 16.0000 - accuracy: 0.9770 - precision: 0.8852 - recall: 0.7714 - auc: 0.9910 - prc: 0.9079 - val_loss: 0.2428 - val_tp: 31.0000 - val_fp: 66.0000 - val_tn: 875.0000 - val_fn: 28.0000 - val_accuracy: 0.9060 - val_precision: 0.3196 - val_recall: 0.5254 - val_auc: 0.8387 - val_prc: 0.2753\n",
      "Epoch 8/1000\n",
      "100/100 [==============================] - 24s 241ms/step - loss: 0.0509 - tp: 59.0000 - fp: 6.0000 - tn: 924.0000 - fn: 11.0000 - accuracy: 0.9830 - precision: 0.9077 - recall: 0.8429 - auc: 0.9932 - prc: 0.9202 - val_loss: 0.2343 - val_tp: 13.0000 - val_fp: 20.0000 - val_tn: 921.0000 - val_fn: 46.0000 - val_accuracy: 0.9340 - val_precision: 0.3939 - val_recall: 0.2203 - val_auc: 0.8217 - val_prc: 0.3011\n",
      "Evaluation on test\n",
      "32/32 [==============================] - 15s 377ms/step - loss: 0.2417 - tp: 9.0000 - fp: 21.0000 - tn: 918.0000 - fn: 52.0000 - accuracy: 0.9270 - precision: 0.3000 - recall: 0.1475 - auc: 0.8027 - prc: 0.2557\n",
      "Returning metrics results\n",
      "Summary: metric loss value 0.24173635244369507\n",
      "Summary: metric tp value 9.0\n",
      "Summary: metric fp value 21.0\n",
      "Summary: metric tn value 918.0\n",
      "Summary: metric fn value 52.0\n",
      "Summary: metric accuracy value 0.9269999861717224\n",
      "Summary: metric precision value 0.30000001192092896\n",
      "Summary: metric recall value 0.14754098653793335\n",
      "Summary: metric auc value 0.8027113080024719\n",
      "Summary: metric prc value 0.25570210814476013\n"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "\n",
    "for balanced in HP_BALANCED.domain.values:\n",
    "    for norm_image in HP_NORM_IMAGES.domain.values:\n",
    "        for network in HP_NETWORKS.domain.values:\n",
    "            for act_func in HP_ACTIVATION_FUNCTIONS.domain.values:\n",
    "                for num_units in HP_NUM_UNITS.domain.values:\n",
    "                    for dropout in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "                        for early_stop in HP_EARLY_STOP.domain.values:\n",
    "                            for optimizer in HP_OPTIMIZER.domain.values:\n",
    "                                hparams = {\n",
    "                                     HP_BALANCED : balanced,\n",
    "                                     HP_NORM_IMAGES: norm_image,\n",
    "                                     HP_NETWORKS: network,\n",
    "                                     HP_ACTIVATION_FUNCTIONS: act_func,\n",
    "                                     HP_NUM_UNITS: num_units,\n",
    "                                     HP_DROPOUT: dropout,\n",
    "                                     HP_EARLY_STOP: early_stop,\n",
    "                                     HP_OPTIMIZER: optimizer\n",
    "                                }\n",
    "                                run_name = \"run-%d\" % session_num\n",
    "                                print('--- Starting trial: %s' % run_name)\n",
    "                                print({h.name: hparams[h] for h in hparams})\n",
    "                                run('logs/hparam_tuning/' + run_name, hparams)\n",
    "                                session_num += 1\n",
    "                                break\n",
    "                            break\n",
    "                        break\n",
    "                    break\n",
    "                break\n",
    "            break\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0042df-1010-4304-b032-4b865893932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142866bb-f1b3-40f5-8ce9-5d6c61800b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664e45b3-99bf-4c6d-94c4-6caadeaa54a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c44331-30fc-4a4d-861a-b2765ce56a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "spectrogram = 'mel' # 'spectrogram', 'mel', 'mfcc', 'mtf'\n",
    "\n",
    "# # root_dir = \"/home/polivares/scratch/Datasets/PowerLineFaults/\"\n",
    "root_dir = '/home/polivares/Dropbox/Work/PostDoc/PowerLineFaults/'\n",
    "\n",
    "X_full_2 = np.load(f\"{root_dir}dataset_{spectrogram}/full/images_full.npy\")\n",
    "# y_full_2 = np.load(f\"{root_dir}dataset_{spectrogram}/full/labels_full.npy\").reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8451debb-f47e-4c54-8665-3b180ed6f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full_2 = normImages(X_full_2)\n",
    "\n",
    "X_train_2 = X_full_2[index_train]\n",
    "X_val_2 = X_full_2[index_val]\n",
    "X_test_2 = X_full_2[index_test]\n",
    "\n",
    "del X_full_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f77624-3f19-4116-b910-1e063024bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "spectrogram = 'mtf' # 'spectrogram', 'mel', 'mfcc', 'mtf'\n",
    "\n",
    "# # root_dir = \"/home/polivares/scratch/Datasets/PowerLineFaults/\"\n",
    "root_dir = '/home/polivares/Dropbox/Work/PostDoc/PowerLineFaults/'\n",
    "\n",
    "X_full_3 = np.load(f\"{root_dir}dataset_{spectrogram}/full/images_full.npy\")\n",
    "# y_full_2 = np.load(f\"{root_dir}dataset_{spectrogram}/full/labels_full.npy\").reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0f2eb4-908e-4215-9020-9541cc8f7b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full_3 = normImages(X_full_3)\n",
    "\n",
    "X_train_3 = X_full_3[index_train]\n",
    "X_val_3 = X_full_3[index_val]\n",
    "X_test_3 = X_full_3[index_test]\n",
    "\n",
    "del X_full_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa31245-d330-4e67-8ad3-e0b49f26e123",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_c = np.concatenate((X_train, X_train_2, X_train_3), axis=3)\n",
    "X_val_c = np.concatenate((X_val, X_val_2, X_val_3), axis=3)\n",
    "X_test_c = np.concatenate((X_test[:1000], X_test_2[:1000], X_test_3[:1000]), axis=3)\n",
    "del X_train, X_train_2, X_train_3, X_val, X_val_2, X_val_3, X_test, X_test_2, X_test_3\n",
    "X_train_c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dba8110",
   "metadata": {},
   "source": [
    "# CNN-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638eaed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de modelo\n",
    "model = models.Sequential()\n",
    "# Capas encargadas de obtener información de la imagen\n",
    "model.add(layers.Conv2D(256, (3, 3), activation='selu', input_shape=(256,256,3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='selu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='selu'))\n",
    "# Capas para la clasificación en base a la información obtenida en \n",
    "# capas anteriores\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='selu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbd3858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(patience = 7)\n",
    "\n",
    "checkpoint_path='CNN-model_mtf.ckpt'\n",
    "# Create a callback that saves the model's weights\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "#                                                  save_weights_only=True,\n",
    "#                                                  verbose=1)\n",
    "\n",
    "METRICS = [\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "      tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "model.compile(loss = 'bce', optimizer = 'adam', metrics=METRICS)\n",
    "history_cnn = model.fit(X_train_c, y_train, epochs=100, \n",
    "                        validation_data=(X_val_c, y_val),\n",
    "                        batch_size=10,\n",
    "                        callbacks=[early_stopping, \n",
    "#                                   cp_callback\n",
    "                                  ]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7655b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_cnn.history['accuracy'], label='accuracy')\n",
    "plt.plot(history_cnn.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19009ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_cnn.history['loss'], label='loss')\n",
    "plt.plot(history_cnn.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2e043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc, val_recall = model.evaluate(X_val,  y_val, verbose=2, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b995ef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = (model.predict(X_val, batch_size=4)>0.5).reshape(1,-1)[0].astype(int)\n",
    "val_real_values = y_val.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e139f0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "matthews_corrcoef(val_real_values, val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7981d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_recall = model.evaluate(X_test,  y_test, verbose=2, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb52d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = (model.predict(X_test, batch_size=4)>0.5).reshape(1,-1)[0].astype(int)\n",
    "test_real_values = y_test.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4963ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matthews_corrcoef(test_real_values, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098dfd40",
   "metadata": {},
   "source": [
    "# CNN 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07118b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creación de modelo\n",
    "# model = models.Sequential()\n",
    "# # Capas encargadas de obtener información de la imagen\n",
    "# model.add(layers.Conv2D(256, (3, 3), activation='selu', input_shape=(256,256,1),kernel_initializer='random_normal',))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# # model.add(layers.Dropout(0.3))\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='selu',kernel_initializer='random_normal',))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# # model.add(layers.Dropout(0.3))\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='selu',kernel_initializer='random_normal',))\n",
    "# # Capas para la clasificación en base a la información obtenida en \n",
    "# # capas anteriores\n",
    "# model.add(layers.Flatten())\n",
    "# model.add(layers.Dense(64, activation='selu',kernel_initializer='random_normal',))\n",
    "# model.add(layers.Dense(1, activation = 'sigmoid',kernel_initializer='random_normal',))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfbb8a6-c985-43ab-852a-d0a5dc08731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de modelo\n",
    "# Armado de nuestra red neuronal\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (5, 5), input_shape=(256, 256, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Conv2D(128, (5, 5), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059ba391",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "      tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(patience = 10, restore_best_weights=True)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "# optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38272aa-a53f-48af-98f7-774a64d83805",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_c, y_train,\n",
    "batch_size=10,\n",
    "epochs=100,\n",
    "verbose=1,\n",
    "validation_data=(X_val_c, y_val),\n",
    "callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b232b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(X_test_c[:1000],  y_test[:1000], verbose=2, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c17d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = (model.predict(X_test, batch_size=4)>0.5).reshape(1,-1)[0].astype(int)\n",
    "test_real_values = y_test.T[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505d104f-85a1-46f0-8fcb-e7a25262dca4",
   "metadata": {},
   "source": [
    "del X_train_c, X_val_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb1d38-8a98-4887-8b9a-a4fe4bb839ae",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88048d9e-ea66-41a6-b013-09c0f4bfca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning\n",
    "# base_model = tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_shape=(256, 256, 3))\n",
    "base_model = tf.keras.applications.EfficientNetB7(include_top=False, weights='imagenet', input_shape=(256, 256, 3))\n",
    "x = tf.keras.layers.Conv2D(64, (3,3), activation='relu')(base_model.output)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(x)\n",
    "# x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(100, activation='relu')(x)\n",
    "# x = tf.keras.layers.Dropout(0.2)(x)\n",
    "# avg = tf.keras.layers.GlobalAveragePooling2D()()\n",
    "output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False # Esto impide que las capas se re entrenen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24183d03-2687-4d86-a16e-90866640043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transfer Learning\n",
    "# base_model = tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_shape=(256, 256, 3))\n",
    "# avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "# output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(avg)\n",
    "\n",
    "# model = tf.keras.Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# for layer in base_model.layers:\n",
    "#     layer.trainable = False # Esto impide que las capas se re entrenen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d010b22d-7d86-4c21-8a69-1df7f78a2173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(patience = 7)\n",
    "\n",
    "METRICS = [\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "      tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "\n",
    "model.compile(loss = 'bce', optimizer = 'adam', metrics=METRICS)\n",
    "history_resnet = model.fit(X_train_c, y_train, epochs=1000, \n",
    "                        validation_data=(X_val_c, y_val),\n",
    "                        batch_size=10,\n",
    "                        callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bc48a9-3e16-4de8-a7fe-3be6b57b1649",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(X_test_c[:2000],  y_test[:2000], verbose=2, batch_size=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
